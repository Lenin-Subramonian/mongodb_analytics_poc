{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eed8fff2-9eaa-4164-9da8-0aca09d27ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/app/data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "cd /app/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f904da4-b434-4f02-8f46-de26f3aceb35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/app/notebooks'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ea54955-6426-441e-874d-e748752e2df3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>account_id</th>\n",
       "      <th>name</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000</td>\n",
       "      <td>Cash</td>\n",
       "      <td>Asset</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000</td>\n",
       "      <td>Accounts Payable</td>\n",
       "      <td>Liability</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   account_id              name       type\n",
       "0        1000              Cash      Asset\n",
       "1        2000  Accounts Payable  Liability"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json('/app/data/accounts.json')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b08d889-07cd-4232-bbac-7ed044a91549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task_id</th>\n",
       "      <th>name</th>\n",
       "      <th>assigned_to</th>\n",
       "      <th>status</th>\n",
       "      <th>due_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T1</td>\n",
       "      <td>Reconcile Cash</td>\n",
       "      <td>Alice</td>\n",
       "      <td>Complete</td>\n",
       "      <td>2025-06-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T2</td>\n",
       "      <td>Review AP</td>\n",
       "      <td>Bob</td>\n",
       "      <td>In Progress</td>\n",
       "      <td>2025-06-06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  task_id            name assigned_to       status    due_date\n",
       "0      T1  Reconcile Cash       Alice     Complete  2025-06-05\n",
       "1      T2       Review AP         Bob  In Progress  2025-06-06"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_json('/app/data/close_tasks.json')\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93d2c4a1-05a0-47cd-b034-9393c3b38b1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entry_id</th>\n",
       "      <th>date</th>\n",
       "      <th>lines</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JE1001</td>\n",
       "      <td>2025-05-31</td>\n",
       "      <td>[{'account_id': '1000', 'debit': 500, 'credit'...</td>\n",
       "      <td>Vendor payment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  entry_id       date                                              lines  \\\n",
       "0   JE1001 2025-05-31  [{'account_id': '1000', 'debit': 500, 'credit'...   \n",
       "\n",
       "      description  \n",
       "0  Vendor payment  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_json('/app/data/journal_entries.json')\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b5046421-1ca6-4c23-90ac-1106c0b6a329",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/22 13:28:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- entry_id: string (nullable = true)\n",
      " |-- lines: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- account_id: string (nullable = true)\n",
      " |    |    |-- credit: long (nullable = true)\n",
      " |    |    |-- debit: long (nullable = true)\n",
      "\n",
      "+----------+--------------+--------+--------------------+\n",
      "|      date|   description|entry_id|               lines|\n",
      "+----------+--------------+--------+--------------------+\n",
      "|2025-05-31|Vendor payment|  JE1001|[{1000, 0, 500}, ...|\n",
      "+----------+--------------+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"ReadMultiLineJSON\").getOrCreate()\n",
    "\n",
    "# Path to your JSON file\n",
    "json_file_path = \"/app/data/journal_entries.json\"\n",
    "\n",
    "# Read the multi-line JSON file into a DataFrame\n",
    "df = spark.read.option(\"multiLine\", \"true\").json(json_file_path)\n",
    "\n",
    "# Show the DataFrame schema and data\n",
    "df.printSchema()\n",
    "df.show()\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f86fc064-9254-41a7-9a8e-fb81f76d27d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- account_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n",
      "+----------+----------------+---------+\n",
      "|account_id|name            |type     |\n",
      "+----------+----------------+---------+\n",
      "|1000      |Cash            |Asset    |\n",
      "|2000      |Accounts Payable|Liability|\n",
      "+----------+----------------+---------+\n",
      "\n",
      "root\n",
      " |-- task_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- assigned_to: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- due_date: date (nullable = true)\n",
      "\n",
      "+-------+--------------+-----------+-----------+----------+\n",
      "|task_id|name          |assigned_to|status     |due_date  |\n",
      "+-------+--------------+-----------+-----------+----------+\n",
      "|T1     |Reconcile Cash|Alice      |Complete   |2025-06-05|\n",
      "|T2     |Review AP     |Bob        |In Progress|2025-06-06|\n",
      "+-------+--------------+-----------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType\n",
    "from pyspark.sql.functions import col, from_json, trim, when, explode, regexp_replace, explode_outer\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"account_closing\").getOrCreate()\n",
    "\n",
    "# Define schema for the DataFrame\n",
    "account_schema = StructType([\n",
    "    StructField(\"account_id\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"type\", StringType(), True)    \t\n",
    "    ])\n",
    "\n",
    "# Define schema for the close_tasks DataFrame\n",
    "close_tasks_schema = StructType([\n",
    "    StructField(\"task_id\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"assigned_to\", StringType(), True),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"due_date\", StringType(), True)\n",
    "])\n",
    "\n",
    "# multiline=True so Spark can parse a top-level JSON array\n",
    "df_account = spark.read.option(\"multiline\", \"true\").schema(account_schema).json('/app/data/accounts.json')\n",
    "\n",
    "# inspect\n",
    "df_account.printSchema()\n",
    "df_account.show(truncate=False)                    \n",
    "\n",
    "# multiline=True so Spark can parse a top-level JSON array\n",
    "df_close_tasks1 = spark.read.option(\"multiline\", \"true\").schema(close_tasks_schema).json('/app/data/close_tasks.json')\n",
    "\n",
    "df_close_tasks = df_close_tasks1.select(\"task_id\", \"name\", \"assigned_to\",\"status\", to_date(\"due_date\", \"yyyy-MM-dd\").alias(\"due_date\"))\n",
    "\n",
    "# inspect\n",
    "df_close_tasks.printSchema()\n",
    "df_close_tasks.show(truncate=False)                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1d2bc004-4ce8-4a8d-80e8-3ce9dec23e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/22 14:55:46 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- entry_id: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- lines: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- account_id: string (nullable = true)\n",
      " |    |    |-- debit: integer (nullable = true)\n",
      " |    |    |-- credit: integer (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      "\n",
      "+--------+----------+--------------------------------+--------------+\n",
      "|entry_id|date      |lines                           |description   |\n",
      "+--------+----------+--------------------------------+--------------+\n",
      "|JE1001  |2025-05-31|[{1000, 500, 0}, {2000, 0, 500}]|Vendor payment|\n",
      "+--------+----------+--------------------------------+--------------+\n",
      "\n",
      "+--------+----------+--------------+----------+-----+------+\n",
      "|entry_id|date      |description   |account_id|debit|credit|\n",
      "+--------+----------+--------------+----------+-----+------+\n",
      "|JE1001  |2025-05-31|Vendor payment|1000      |500  |0     |\n",
      "|JE1001  |2025-05-31|Vendor payment|2000      |0    |500   |\n",
      "+--------+----------+--------------+----------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType,IntegerType\n",
    "from pyspark.sql.functions import col, from_json, trim, when, explode, regexp_replace, explode_outer, to_date\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ReadJSONWithSchema\").getOrCreate()\n",
    "\n",
    "# Path to your JSON file\n",
    "json_file_path = \"/app/data/journal_entries.json\"\n",
    "\n",
    "# Define schema for journal_entries_data (JSON array)\n",
    "journal_schema = StructType([\n",
    "    StructField(\"entry_id\", StringType(), nullable=True),\n",
    "    StructField(\"date\", StringType(), nullable=True),\n",
    "    StructField(\"lines\", ArrayType(\n",
    "        StructType([\n",
    "            StructField(\"account_id\", StringType(), nullable=True),\n",
    "            StructField(\"debit\", IntegerType(), nullable=True),\n",
    "            StructField(\"credit\", IntegerType(), nullable=True),\n",
    "        ])\n",
    "    ), nullable=True),\n",
    "    StructField(\"description\", StringType(), nullable=True)\n",
    "])\n",
    "\n",
    "# multiline=True so Spark can parse a top-level JSON array\n",
    "df = spark.read.option(\"multiline\", \"true\").schema(journal_schema).json(json_file_path)\n",
    "\n",
    "# inspect\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "# explode lines to inspect line-level fields\n",
    "df_lines = df.select(\"entry_id\", \"date\", \"description\", explode(col(\"lines\")).alias(\"line\"))\n",
    "df_lines.select(\"entry_id\", to_date(\"date\", \"yyyy-MM-dd\").alias(\"date\"),\n",
    "                \"description\",\n",
    "                col(\"line.account_id\").alias(\"account_id\"),\n",
    "                col(\"line.debit\").alias(\"debit\"),\n",
    "                col(\"line.credit\").alias(\"credit\")\n",
    "               ).show(truncate=False)\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "be3a50ee-e6c7-4042-82e9-561edd3dfbe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+---------+\n",
      "|account_id|name            |type     |\n",
      "+----------+----------------+---------+\n",
      "|1000      |Cash            |Asset    |\n",
      "|2000      |Accounts Payable|Liability|\n",
      "+----------+----------------+---------+\n",
      "\n",
      "+-------+-----------------+----------------+---------+\n",
      "|summary|       account_id|            name|     type|\n",
      "+-------+-----------------+----------------+---------+\n",
      "|  count|                2|               2|        2|\n",
      "|   mean|           1500.0|            NULL|     NULL|\n",
      "| stddev|707.1067811865476|            NULL|     NULL|\n",
      "|    min|             1000|Accounts Payable|    Asset|\n",
      "|    max|             2000|            Cash|Liability|\n",
      "+-------+-----------------+----------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType\n",
    "from pyspark.sql.functions import col, from_json, trim, when, explode, regexp_replace, explode_outer\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"account_closing\").getOrCreate()\n",
    "\n",
    "# Define schema for the accounts DataFrame\n",
    "account_schema = StructType([\n",
    "    StructField(\"account_id\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"type\", StringType(), True)    \t\n",
    "    ])\n",
    "\n",
    "# Define schema for the close_tasks DataFrame\n",
    "close_tasks_schema = StructType([\n",
    "    StructField(\"task_id\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"assigned_to\", StringType(), True),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"due_date\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Define schema for journal_entries_data (JSON array)\n",
    "journal_schema = StructType([\n",
    "    StructField(\"entry_id\", StringType(), nullable=True),\n",
    "    StructField(\"date\", StringType(), nullable=True),\n",
    "    StructField(\"lines\", ArrayType(\n",
    "        StructType([\n",
    "            StructField(\"account_id\", StringType(), nullable=True),\n",
    "            StructField(\"debit\", IntegerType(), nullable=True),\n",
    "            StructField(\"credit\", IntegerType(), nullable=True),\n",
    "        ])\n",
    "    ), nullable=True),\n",
    "    StructField(\"description\", StringType(), nullable=True)\n",
    "])\n",
    "\n",
    "# multiline=True so Spark can parse a top-level JSON array\n",
    "df_account = spark.read.option(\"multiline\", \"true\").schema(account_schema).json('/app/data/accounts.json')                  \n",
    "\n",
    "# multiline=True so Spark can parse a top-level JSON array\n",
    "df_close_tasks = spark.read.option(\"multiline\", \"true\").schema(close_tasks_schema).json('/app/data/close_tasks.json')\n",
    "\n",
    "# multiline=True so Spark can parse a top-level JSON array\n",
    "df_journal = spark.read.option(\"multiline\", \"true\").schema(journal_schema).json('/app/data/journal_entries.json')\n",
    "\n",
    "\n",
    "df_close_tasks = df_close_tasks.select(\"task_id\", \"name\", \"assigned_to\",\"status\", to_date(\"due_date\", \"yyyy-MM-dd\").alias(\"due_date\"))\n",
    "\n",
    "# explode lines to inspect line-level fields\n",
    "df_lines = df_journal.select(\"entry_id\", \"date\", \"description\", explode(col(\"lines\")).alias(\"line\"))\n",
    "df_flattened = df_lines.select(\"entry_id\", to_date(\"date\", \"yyyy-MM-dd\").alias(\"date\"),\n",
    "                \"description\",\n",
    "                col(\"line.account_id\").alias(\"account_id\"),\n",
    "                col(\"line.debit\").alias(\"debit\"),\n",
    "                col(\"line.credit\").alias(\"credit\")\n",
    "               )\n",
    "\n",
    "# # Register DataFrame as a SQL temporary view\n",
    "df_account.createOrReplaceTempView(\"accounts\")\n",
    "\n",
    "# # Register DataFrame as a SQL temporary view\n",
    "df_close_tasks.createOrReplaceTempView(\"close_tasks\")\n",
    "\n",
    "# # Register DataFrame as a SQL temporary view\n",
    "df_flattened.createOrReplaceTempView(\"journal_entries\")\n",
    "\n",
    "# # Run a Spark SQL query\n",
    "df_result = spark.sql(\"SELECT * FROM accounts\")\n",
    "\n",
    "# # Show the results\n",
    "df_result.show(truncate=False)\n",
    "\n",
    "df_result.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "44781934-7afd-4c26-b488-aec0ffd41d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+---------+\n",
      "|account_id|name            |type     |\n",
      "+----------+----------------+---------+\n",
      "|1000      |Cash            |Asset    |\n",
      "|2000      |Accounts Payable|Liability|\n",
      "+----------+----------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Run a Spark SQL query\n",
    "df1 = spark.sql(\"SELECT * FROM accounts\")\n",
    "\n",
    "# # Show the results\n",
    "df1.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "48e9841e-3818-4561-ae97-1e4c4fc0679e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+-----------+-----------+----------+\n",
      "|task_id|name          |assigned_to|status     |due_date  |\n",
      "+-------+--------------+-----------+-----------+----------+\n",
      "|T1     |Reconcile Cash|Alice      |Complete   |2025-06-05|\n",
      "|T2     |Review AP     |Bob        |In Progress|2025-06-06|\n",
      "+-------+--------------+-----------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Run a Spark SQL query\n",
    "df2 = spark.sql(\"SELECT * FROM close_tasks\")\n",
    "\n",
    "# # Show the results\n",
    "df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b8e63471-cc55-4619-b252-fe45d5c32982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------------+----------+-----+------+\n",
      "|entry_id|date      |description   |account_id|debit|credit|\n",
      "+--------+----------+--------------+----------+-----+------+\n",
      "|JE1001  |2025-05-31|Vendor payment|1000      |500  |0     |\n",
      "|JE1001  |2025-05-31|Vendor payment|2000      |0    |500   |\n",
      "+--------+----------+--------------+----------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Run a Spark SQL query\n",
    "df3 = spark.sql(\"SELECT * FROM journal_entries\")\n",
    "\n",
    "# # Show the results\n",
    "df3.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f965a45d-3bd1-4650-90ee-3d85db4c7924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Order View (journal_entry_detail_vw)\n",
    "\n",
    "## Apply below transformations: \n",
    "    # 1. Join account and journal entry table to give a full picture of the journal entry details at each line granularity. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "857f6b27-79be-4528-a5e9-a6951ec3678e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------------+----------+----------------+------------+-----+------+\n",
      "|entry_id|date      |description   |account_id|account_name    |account_type|debit|credit|\n",
      "+--------+----------+--------------+----------+----------------+------------+-----+------+\n",
      "|JE1001  |2025-05-31|Vendor payment|1000      |Cash            |Asset       |500  |0     |\n",
      "|JE1001  |2025-05-31|Vendor payment|2000      |Accounts Payable|Liability   |0    |500   |\n",
      "+--------+----------+--------------+----------+----------------+------------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SQL query for the view\n",
    "sql_query = \"\"\"\n",
    "    select \n",
    "        je.entry_id, \n",
    "        je.date, \n",
    "        je.description,\n",
    "        je.account_id, \n",
    "        a.name as account_name,\n",
    "        a.type as account_type,\n",
    "        je.debit,\n",
    "        je.credit\n",
    "    from journal_entries je inner join accounts a on je.account_id = a.account_id\n",
    "\"\"\"\n",
    "# Execute SQL query\n",
    "df_trans = spark.sql(sql_query)\n",
    "\n",
    "df_trans.createOrReplaceTempView(\"journal_entry_detail_vw\")\n",
    "\n",
    "df_trans.show(truncate=False)\n",
    "\n",
    "# df_trans.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712fa68d-9214-43ee-9706-e80ba297cc86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3517ffe9-a5aa-454a-9dc7-4c2a686c7287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 2 documents from 'accounts'\n",
      "Downloaded 2 documents from 'close_tasks'\n",
      "Downloaded 1 documents from 'journal_entries'\n",
      "Docs from collection: accounts\n",
      "[{'_id': ObjectId('68ca9a71193e5d257e046f1c'), 'account_id': '1000', 'name': 'Cash', 'type': 'Asset'}, {'_id': ObjectId('68ca9a71193e5d257e046f1d'), 'account_id': '2000', 'name': 'Accounts Payable', 'type': 'Liability'}]\n",
      "-------------------------------\n",
      "Docs from collection: close_tasks\n",
      "[{'_id': ObjectId('68ca9b35193e5d257e046f25'), 'task_id': 'T1', 'name': 'Reconcile Cash', 'assigned_to': 'Alice', 'status': 'Complete', 'due_date': '2025-06-05'}, {'_id': ObjectId('68ca9b35193e5d257e046f26'), 'task_id': 'T2', 'name': 'Review AP', 'assigned_to': 'Bob', 'status': 'In Progress', 'due_date': '2025-06-06'}]\n",
      "-------------------------------\n",
      "Docs from collection: journal_entries\n",
      "[{'_id': ObjectId('68ca9aca193e5d257e046f22'), 'entry_id': 'JE1001', 'date': '2025-05-31', 'lines': [{'account_id': '1000', 'debit': 500, 'credit': 0}, {'account_id': '2000', 'debit': 0, 'credit': 500}], 'description': 'Vendor payment'}]\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "\n",
    "mongo_uri = \"mongodb+srv://ls_db_user:s47tBnWikllAoe3k@democluster0.j777gry.mongodb.net/?retryWrites=true&w=majority&appName=DemoCluster0\"\n",
    "s3_bucket = \"s3://fq-app-analytics-bucket-1/iceberg-warehouse/ecommerce_db_raw/\"\n",
    "\n",
    "client = MongoClient(mongo_uri)\n",
    "db = client[\"FQ_App\"]\n",
    "# coll = db[\"accounts\"]\n",
    "collections = [\"accounts\", \"close_tasks\", \"journal_entries\"]  # Replace with your collection names\n",
    "        \n",
    "# Get total count for progress tracking\n",
    "# total_docs = coll.count_documents({})\n",
    "# print(f\"Total documents to process: {total_docs}\")\n",
    "\n",
    "data_from_collections = {}  # Dictionary to store data from each collection\n",
    "\n",
    "for collection_name in collections:\n",
    "    collection = db[collection_name]\n",
    "    documents = list(collection.find({}))  # Retrieve all documents and convert cursor to a list\n",
    "    data_from_collections[collection_name] = documents\n",
    "    print(f\"Downloaded {len(documents)} documents from '{collection_name}'\")\n",
    "\n",
    "# Now, 'data_from_collections' holds the data from all specified collections\n",
    "# For example, to access data from 'collection1':\n",
    "for collection in collections:\n",
    "    print('Docs from collection:', collection)\n",
    "    print(data_from_collections[collection])\n",
    "    print(\"-------------------------------\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cbd08b-1044-4edf-9ff7-83257cf51d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the JSON data from S3, which you would have uploaded in a separate step\n",
    "orders_df = spark.read.json(\"s3a://fq-app-analytics-bucket-1/iceberg-warehouse/raw_json/orders.json\")\n",
    "\n",
    "# Write the DataFrame as an Iceberg table in the ecommerce_db_raw database\n",
    "orders_df.write.format(\"iceberg\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"glue_catalog.ecommerce_db_raw.orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "adcfa516-4506-4bdd-b16e-b6cc93d58e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark version: 3.5.3\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "print(f'PySpark version: {pyspark.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8adc014a-1509-4997-9da9-e8fa9334c82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3, os\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = 'AKIA5QMIH4RHVEXR3P4F'\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = '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'\n",
    "os.environ['AWS_REGION'] = 'us-east-2'\n",
    "os.environ['S3_BUCKET'] ='fq-app-analytics-bucket-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "78b1dc3d-6048-49ac-9110-a2c49ffd51cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'os' has no attribute 'S3_BUCKET'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAWS_REGION\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mus-east-2\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      6\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mS3_BUCKET\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfq-app-analytics-bucket-1\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mS3_BUCKET\u001b[49m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# # s3_bucket = \"s3://fq-app-analytics-bucket-1/iceberg-warehouse/ecommerce_db_raw/\"\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# bucket_name = 'fq-app-analytics-bucket-1'\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# object_key = 'iceberg-warehouse/ecommerce_db_raw/'\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# except Exception as e:\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m#     print('S3 check failed:', e)\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'os' has no attribute 'S3_BUCKET'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(os.S3_BUCKET)\n",
    "\n",
    "\n",
    "# # s3_bucket = \"s3://fq-app-analytics-bucket-1/iceberg-warehouse/ecommerce_db_raw/\"\n",
    "# bucket_name = 'fq-app-analytics-bucket-1'\n",
    "# object_key = 'iceberg-warehouse/ecommerce_db_raw/'\n",
    "# s3 = boto3.client('s3')\n",
    "# # bucket = os.getenv('S3_BUCKET')\n",
    "\n",
    "# import boto3\n",
    "\n",
    "# s3_client = boto3.client('s3')\n",
    "# response = s3_client.list_objects_v2(\n",
    "#     Bucket=s3_bucket,\n",
    "#     Prefix=f\"iceberg-warehouse/ecommerce_db_raw/\"\n",
    "# )\n",
    "\n",
    "# for obj in response.get('Contents', []):\n",
    "#     print(obj['Key'])\n",
    "\n",
    "\n",
    "\n",
    "# response = s3.list_objects_v2(Bucket=bucket_name, Prefix=object_key)\n",
    "# print(response)\n",
    "\n",
    "# try:\n",
    "#     objects = s3.list_objects_v2(Bucket=s3_bucket, Prefix='data/')\n",
    "#     print('S3 objects:', [obj['Key'] for obj in objects.get('Contents', [])])\n",
    "# except Exception as e:\n",
    "#     print('S3 check failed:', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f7b5f2-5999-4e86-9a61-2d3bd3ac4d09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
