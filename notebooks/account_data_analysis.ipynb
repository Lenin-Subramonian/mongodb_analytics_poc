{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eed8fff2-9eaa-4164-9da8-0aca09d27ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/app/data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "cd /app/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f904da4-b434-4f02-8f46-de26f3aceb35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/app/notebooks'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ea54955-6426-441e-874d-e748752e2df3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>account_id</th>\n",
       "      <th>name</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000</td>\n",
       "      <td>Cash</td>\n",
       "      <td>Asset</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000</td>\n",
       "      <td>Accounts Payable</td>\n",
       "      <td>Liability</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   account_id              name       type\n",
       "0        1000              Cash      Asset\n",
       "1        2000  Accounts Payable  Liability"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json('/app/data/accounts.json')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b08d889-07cd-4232-bbac-7ed044a91549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task_id</th>\n",
       "      <th>name</th>\n",
       "      <th>assigned_to</th>\n",
       "      <th>status</th>\n",
       "      <th>due_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T1</td>\n",
       "      <td>Reconcile Cash</td>\n",
       "      <td>Alice</td>\n",
       "      <td>Complete</td>\n",
       "      <td>2025-06-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T2</td>\n",
       "      <td>Review AP</td>\n",
       "      <td>Bob</td>\n",
       "      <td>In Progress</td>\n",
       "      <td>2025-06-06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  task_id            name assigned_to       status    due_date\n",
       "0      T1  Reconcile Cash       Alice     Complete  2025-06-05\n",
       "1      T2       Review AP         Bob  In Progress  2025-06-06"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_json('/app/data/close_tasks.json')\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93d2c4a1-05a0-47cd-b034-9393c3b38b1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entry_id</th>\n",
       "      <th>date</th>\n",
       "      <th>lines</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JE1001</td>\n",
       "      <td>2025-05-31</td>\n",
       "      <td>[{'account_id': '1000', 'debit': 500, 'credit'...</td>\n",
       "      <td>Vendor payment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  entry_id       date                                              lines  \\\n",
       "0   JE1001 2025-05-31  [{'account_id': '1000', 'debit': 500, 'credit'...   \n",
       "\n",
       "      description  \n",
       "0  Vendor payment  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_json('/app/data/journal_entries.json')\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b5046421-1ca6-4c23-90ac-1106c0b6a329",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/22 13:28:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- entry_id: string (nullable = true)\n",
      " |-- lines: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- account_id: string (nullable = true)\n",
      " |    |    |-- credit: long (nullable = true)\n",
      " |    |    |-- debit: long (nullable = true)\n",
      "\n",
      "+----------+--------------+--------+--------------------+\n",
      "|      date|   description|entry_id|               lines|\n",
      "+----------+--------------+--------+--------------------+\n",
      "|2025-05-31|Vendor payment|  JE1001|[{1000, 0, 500}, ...|\n",
      "+----------+--------------+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"ReadMultiLineJSON\").getOrCreate()\n",
    "\n",
    "# Path to your JSON file\n",
    "json_file_path = \"/app/data/journal_entries.json\"\n",
    "\n",
    "# Read the multi-line JSON file into a DataFrame\n",
    "df = spark.read.option(\"multiLine\", \"true\").json(json_file_path)\n",
    "\n",
    "# Show the DataFrame schema and data\n",
    "df.printSchema()\n",
    "df.show()\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f86fc064-9254-41a7-9a8e-fb81f76d27d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- account_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n",
      "+----------+----------------+---------+\n",
      "|account_id|name            |type     |\n",
      "+----------+----------------+---------+\n",
      "|1000      |Cash            |Asset    |\n",
      "|2000      |Accounts Payable|Liability|\n",
      "+----------+----------------+---------+\n",
      "\n",
      "root\n",
      " |-- task_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- assigned_to: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- due_date: date (nullable = true)\n",
      "\n",
      "+-------+--------------+-----------+-----------+----------+\n",
      "|task_id|name          |assigned_to|status     |due_date  |\n",
      "+-------+--------------+-----------+-----------+----------+\n",
      "|T1     |Reconcile Cash|Alice      |Complete   |2025-06-05|\n",
      "|T2     |Review AP     |Bob        |In Progress|2025-06-06|\n",
      "+-------+--------------+-----------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType\n",
    "from pyspark.sql.functions import col, from_json, trim, when, explode, regexp_replace, explode_outer\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"account_closing\").getOrCreate()\n",
    "\n",
    "# Define schema for the DataFrame\n",
    "account_schema = StructType([\n",
    "    StructField(\"account_id\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"type\", StringType(), True)    \t\n",
    "    ])\n",
    "\n",
    "# Define schema for the close_tasks DataFrame\n",
    "close_tasks_schema = StructType([\n",
    "    StructField(\"task_id\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"assigned_to\", StringType(), True),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"due_date\", StringType(), True)\n",
    "])\n",
    "\n",
    "# multiline=True so Spark can parse a top-level JSON array\n",
    "df_account = spark.read.option(\"multiline\", \"true\").schema(account_schema).json('/app/data/accounts.json')\n",
    "\n",
    "# inspect\n",
    "df_account.printSchema()\n",
    "df_account.show(truncate=False)                    \n",
    "\n",
    "# multiline=True so Spark can parse a top-level JSON array\n",
    "df_close_tasks1 = spark.read.option(\"multiline\", \"true\").schema(close_tasks_schema).json('/app/data/close_tasks.json')\n",
    "\n",
    "df_close_tasks = df_close_tasks1.select(\"task_id\", \"name\", \"assigned_to\",\"status\", to_date(\"due_date\", \"yyyy-MM-dd\").alias(\"due_date\"))\n",
    "\n",
    "# inspect\n",
    "df_close_tasks.printSchema()\n",
    "df_close_tasks.show(truncate=False)                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1d2bc004-4ce8-4a8d-80e8-3ce9dec23e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/22 14:55:46 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- entry_id: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- lines: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- account_id: string (nullable = true)\n",
      " |    |    |-- debit: integer (nullable = true)\n",
      " |    |    |-- credit: integer (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      "\n",
      "+--------+----------+--------------------------------+--------------+\n",
      "|entry_id|date      |lines                           |description   |\n",
      "+--------+----------+--------------------------------+--------------+\n",
      "|JE1001  |2025-05-31|[{1000, 500, 0}, {2000, 0, 500}]|Vendor payment|\n",
      "+--------+----------+--------------------------------+--------------+\n",
      "\n",
      "+--------+----------+--------------+----------+-----+------+\n",
      "|entry_id|date      |description   |account_id|debit|credit|\n",
      "+--------+----------+--------------+----------+-----+------+\n",
      "|JE1001  |2025-05-31|Vendor payment|1000      |500  |0     |\n",
      "|JE1001  |2025-05-31|Vendor payment|2000      |0    |500   |\n",
      "+--------+----------+--------------+----------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType,IntegerType\n",
    "from pyspark.sql.functions import col, from_json, trim, when, explode, regexp_replace, explode_outer, to_date\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ReadJSONWithSchema\").getOrCreate()\n",
    "\n",
    "# Path to your JSON file\n",
    "json_file_path = \"/app/data/journal_entries.json\"\n",
    "\n",
    "# Define schema for journal_entries_data (JSON array)\n",
    "journal_schema = StructType([\n",
    "    StructField(\"entry_id\", StringType(), nullable=True),\n",
    "    StructField(\"date\", StringType(), nullable=True),\n",
    "    StructField(\"lines\", ArrayType(\n",
    "        StructType([\n",
    "            StructField(\"account_id\", StringType(), nullable=True),\n",
    "            StructField(\"debit\", IntegerType(), nullable=True),\n",
    "            StructField(\"credit\", IntegerType(), nullable=True),\n",
    "        ])\n",
    "    ), nullable=True),\n",
    "    StructField(\"description\", StringType(), nullable=True)\n",
    "])\n",
    "\n",
    "# multiline=True so Spark can parse a top-level JSON array\n",
    "df = spark.read.option(\"multiline\", \"true\").schema(journal_schema).json(json_file_path)\n",
    "\n",
    "# inspect\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "# explode lines to inspect line-level fields\n",
    "df_lines = df.select(\"entry_id\", \"date\", \"description\", explode(col(\"lines\")).alias(\"line\"))\n",
    "df_lines.select(\"entry_id\", to_date(\"date\", \"yyyy-MM-dd\").alias(\"date\"),\n",
    "                \"description\",\n",
    "                col(\"line.account_id\").alias(\"account_id\"),\n",
    "                col(\"line.debit\").alias(\"debit\"),\n",
    "                col(\"line.credit\").alias(\"credit\")\n",
    "               ).show(truncate=False)\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "be3a50ee-e6c7-4042-82e9-561edd3dfbe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+---------+\n",
      "|account_id|name            |type     |\n",
      "+----------+----------------+---------+\n",
      "|1000      |Cash            |Asset    |\n",
      "|2000      |Accounts Payable|Liability|\n",
      "+----------+----------------+---------+\n",
      "\n",
      "+-------+-----------------+----------------+---------+\n",
      "|summary|       account_id|            name|     type|\n",
      "+-------+-----------------+----------------+---------+\n",
      "|  count|                2|               2|        2|\n",
      "|   mean|           1500.0|            NULL|     NULL|\n",
      "| stddev|707.1067811865476|            NULL|     NULL|\n",
      "|    min|             1000|Accounts Payable|    Asset|\n",
      "|    max|             2000|            Cash|Liability|\n",
      "+-------+-----------------+----------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType\n",
    "from pyspark.sql.functions import col, from_json, trim, when, explode, regexp_replace, explode_outer\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"account_closing\").getOrCreate()\n",
    "\n",
    "# Define schema for the accounts DataFrame\n",
    "account_schema = StructType([\n",
    "    StructField(\"account_id\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"type\", StringType(), True)    \t\n",
    "    ])\n",
    "\n",
    "# Define schema for the close_tasks DataFrame\n",
    "close_tasks_schema = StructType([\n",
    "    StructField(\"task_id\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"assigned_to\", StringType(), True),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"due_date\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Define schema for journal_entries_data (JSON array)\n",
    "journal_schema = StructType([\n",
    "    StructField(\"entry_id\", StringType(), nullable=True),\n",
    "    StructField(\"date\", StringType(), nullable=True),\n",
    "    StructField(\"lines\", ArrayType(\n",
    "        StructType([\n",
    "            StructField(\"account_id\", StringType(), nullable=True),\n",
    "            StructField(\"debit\", IntegerType(), nullable=True),\n",
    "            StructField(\"credit\", IntegerType(), nullable=True),\n",
    "        ])\n",
    "    ), nullable=True),\n",
    "    StructField(\"description\", StringType(), nullable=True)\n",
    "])\n",
    "\n",
    "# multiline=True so Spark can parse a top-level JSON array\n",
    "df_account = spark.read.option(\"multiline\", \"true\").schema(account_schema).json('/app/data/accounts.json')                  \n",
    "\n",
    "# multiline=True so Spark can parse a top-level JSON array\n",
    "df_close_tasks = spark.read.option(\"multiline\", \"true\").schema(close_tasks_schema).json('/app/data/close_tasks.json')\n",
    "\n",
    "# multiline=True so Spark can parse a top-level JSON array\n",
    "df_journal = spark.read.option(\"multiline\", \"true\").schema(journal_schema).json('/app/data/journal_entries.json')\n",
    "\n",
    "\n",
    "df_close_tasks = df_close_tasks.select(\"task_id\", \"name\", \"assigned_to\",\"status\", to_date(\"due_date\", \"yyyy-MM-dd\").alias(\"due_date\"))\n",
    "\n",
    "# explode lines to inspect line-level fields\n",
    "df_lines = df_journal.select(\"entry_id\", \"date\", \"description\", explode(col(\"lines\")).alias(\"line\"))\n",
    "df_flattened = df_lines.select(\"entry_id\", to_date(\"date\", \"yyyy-MM-dd\").alias(\"date\"),\n",
    "                \"description\",\n",
    "                col(\"line.account_id\").alias(\"account_id\"),\n",
    "                col(\"line.debit\").alias(\"debit\"),\n",
    "                col(\"line.credit\").alias(\"credit\")\n",
    "               )\n",
    "\n",
    "# # Register DataFrame as a SQL temporary view\n",
    "df_account.createOrReplaceTempView(\"accounts\")\n",
    "\n",
    "# # Register DataFrame as a SQL temporary view\n",
    "df_close_tasks.createOrReplaceTempView(\"close_tasks\")\n",
    "\n",
    "# # Register DataFrame as a SQL temporary view\n",
    "df_flattened.createOrReplaceTempView(\"journal_entries\")\n",
    "\n",
    "# # Run a Spark SQL query\n",
    "df_result = spark.sql(\"SELECT * FROM accounts\")\n",
    "\n",
    "# # Show the results\n",
    "df_result.show(truncate=False)\n",
    "\n",
    "df_result.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "44781934-7afd-4c26-b488-aec0ffd41d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+---------+\n",
      "|account_id|name            |type     |\n",
      "+----------+----------------+---------+\n",
      "|1000      |Cash            |Asset    |\n",
      "|2000      |Accounts Payable|Liability|\n",
      "+----------+----------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Run a Spark SQL query\n",
    "df1 = spark.sql(\"SELECT * FROM accounts\")\n",
    "\n",
    "# # Show the results\n",
    "df1.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "48e9841e-3818-4561-ae97-1e4c4fc0679e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+-----------+-----------+----------+\n",
      "|task_id|name          |assigned_to|status     |due_date  |\n",
      "+-------+--------------+-----------+-----------+----------+\n",
      "|T1     |Reconcile Cash|Alice      |Complete   |2025-06-05|\n",
      "|T2     |Review AP     |Bob        |In Progress|2025-06-06|\n",
      "+-------+--------------+-----------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Run a Spark SQL query\n",
    "df2 = spark.sql(\"SELECT * FROM close_tasks\")\n",
    "\n",
    "# # Show the results\n",
    "df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b8e63471-cc55-4619-b252-fe45d5c32982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------------+----------+-----+------+\n",
      "|entry_id|date      |description   |account_id|debit|credit|\n",
      "+--------+----------+--------------+----------+-----+------+\n",
      "|JE1001  |2025-05-31|Vendor payment|1000      |500  |0     |\n",
      "|JE1001  |2025-05-31|Vendor payment|2000      |0    |500   |\n",
      "+--------+----------+--------------+----------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Run a Spark SQL query\n",
    "df3 = spark.sql(\"SELECT * FROM journal_entries\")\n",
    "\n",
    "# # Show the results\n",
    "df3.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f965a45d-3bd1-4650-90ee-3d85db4c7924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Order View (journal_entry_detail_vw)\n",
    "\n",
    "## Apply below transformations: \n",
    "    # 1. Join account and journal entry table to give a full picture of the journal entry details at each line granularity. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "857f6b27-79be-4528-a5e9-a6951ec3678e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------------+----------+----------------+------------+-----+------+\n",
      "|entry_id|date      |description   |account_id|account_name    |account_type|debit|credit|\n",
      "+--------+----------+--------------+----------+----------------+------------+-----+------+\n",
      "|JE1001  |2025-05-31|Vendor payment|1000      |Cash            |Asset       |500  |0     |\n",
      "|JE1001  |2025-05-31|Vendor payment|2000      |Accounts Payable|Liability   |0    |500   |\n",
      "+--------+----------+--------------+----------+----------------+------------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SQL query for the view\n",
    "sql_query = \"\"\"\n",
    "    select \n",
    "        je.entry_id, \n",
    "        je.date, \n",
    "        je.description,\n",
    "        je.account_id, \n",
    "        a.name as account_name,\n",
    "        a.type as account_type,\n",
    "        je.debit,\n",
    "        je.credit\n",
    "    from journal_entries je inner join accounts a on je.account_id = a.account_id\n",
    "\"\"\"\n",
    "# Execute SQL query\n",
    "df_trans = spark.sql(sql_query)\n",
    "\n",
    "df_trans.createOrReplaceTempView(\"journal_entry_detail_vw\")\n",
    "\n",
    "df_trans.show(truncate=False)\n",
    "\n",
    "# df_trans.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712fa68d-9214-43ee-9706-e80ba297cc86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3517ffe9-a5aa-454a-9dc7-4c2a686c7287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 2 documents from 'accounts'\n",
      "Downloaded 2 documents from 'close_tasks'\n",
      "Downloaded 1 documents from 'journal_entries'\n",
      "Docs from collection: accounts\n",
      "[{'_id': ObjectId('68ca9a71193e5d257e046f1c'), 'account_id': '1000', 'name': 'Cash', 'type': 'Asset'}, {'_id': ObjectId('68ca9a71193e5d257e046f1d'), 'account_id': '2000', 'name': 'Accounts Payable', 'type': 'Liability'}]\n",
      "-------------------------------\n",
      "Docs from collection: close_tasks\n",
      "[{'_id': ObjectId('68ca9b35193e5d257e046f25'), 'task_id': 'T1', 'name': 'Reconcile Cash', 'assigned_to': 'Alice', 'status': 'Complete', 'due_date': '2025-06-05'}, {'_id': ObjectId('68ca9b35193e5d257e046f26'), 'task_id': 'T2', 'name': 'Review AP', 'assigned_to': 'Bob', 'status': 'In Progress', 'due_date': '2025-06-06'}]\n",
      "-------------------------------\n",
      "Docs from collection: journal_entries\n",
      "[{'_id': ObjectId('68ca9aca193e5d257e046f22'), 'entry_id': 'JE1001', 'date': '2025-05-31', 'lines': [{'account_id': '1000', 'debit': 500, 'credit': 0}, {'account_id': '2000', 'debit': 0, 'credit': 500}], 'description': 'Vendor payment'}]\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "\n",
    "mongo_uri = \"mongodb+srv://ls_db_user:s47tBnWikllAoe3k@democluster0.j777gry.mongodb.net/?retryWrites=true&w=majority&appName=DemoCluster0\"\n",
    "s3_bucket = \"s3://fq-app-analytics-bucket-1/iceberg-warehouse/ecommerce_db_raw/\"\n",
    "\n",
    "client = MongoClient(mongo_uri)\n",
    "db = client[\"FQ_App\"]\n",
    "# coll = db[\"accounts\"]\n",
    "collections = [\"accounts\", \"close_tasks\", \"journal_entries\"]  # Replace with your collection names\n",
    "        \n",
    "# Get total count for progress tracking\n",
    "# total_docs = coll.count_documents({})\n",
    "# print(f\"Total documents to process: {total_docs}\")\n",
    "\n",
    "data_from_collections = {}  # Dictionary to store data from each collection\n",
    "\n",
    "for collection_name in collections:\n",
    "    collection = db[collection_name]\n",
    "    documents = list(collection.find({}))  # Retrieve all documents and convert cursor to a list\n",
    "    data_from_collections[collection_name] = documents\n",
    "    print(f\"Downloaded {len(documents)} documents from '{collection_name}'\")\n",
    "\n",
    "# Now, 'data_from_collections' holds the data from all specified collections\n",
    "# For example, to access data from 'collection1':\n",
    "for collection in collections:\n",
    "    print('Docs from collection:', collection)\n",
    "    print(data_from_collections[collection])\n",
    "    print(\"-------------------------------\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cbd08b-1044-4edf-9ff7-83257cf51d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the JSON data from S3, which you would have uploaded in a separate step\n",
    "orders_df = spark.read.json(\"s3a://fq-app-analytics-bucket-1/iceberg-warehouse/raw_json/orders.json\")\n",
    "\n",
    "# Write the DataFrame as an Iceberg table in the ecommerce_db_raw database\n",
    "orders_df.write.format(\"iceberg\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"glue_catalog.ecommerce_db_raw.orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "302138e1-8c95-4e01-9c4f-3f8d8854bdd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/app/notebooks'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adcfa516-4506-4bdd-b16e-b6cc93d58e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark version: 3.5.3\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "print(f'PySpark version: {pyspark.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f726490-b68c-488c-aa08-012d1219dc8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.iceberg#iceberg-spark-runtime-3.5_2.12 added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "software.amazon.awssdk#glue added as a dependency\n",
      "software.amazon.awssdk#s3 added as a dependency\n",
      "software.amazon.awssdk#sts added as a dependency\n",
      "software.amazon.awssdk#dynamodb added as a dependency\n",
      "software.amazon.awssdk#kms added as a dependency\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-9939de60-eec6-466c-9379-ef2563aa3003;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.5.2 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.6 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.367 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.1.3.Final in central\n",
      "\tfound software.amazon.awssdk#glue;2.20.137 in central\n",
      "\tfound software.amazon.awssdk#aws-json-protocol;2.20.137 in central\n",
      "\tfound software.amazon.awssdk#aws-core;2.20.137 in central\n",
      "\tfound software.amazon.awssdk#annotations;2.20.137 in central\n",
      "\tfound software.amazon.awssdk#regions;2.20.137 in central\n",
      "\tfound software.amazon.awssdk#utils;2.20.137 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound software.amazon.awssdk#sdk-core;2.20.137 in central\n",
      "\tfound software.amazon.awssdk#http-client-spi;2.20.137 in central\n",
      "\tfound software.amazon.awssdk#metrics-spi;2.20.137 in central\n",
      "\tfound software.amazon.awssdk#endpoints-spi;2.20.137 in central\n",
      "\tfound software.amazon.awssdk#profiles;2.20.137 in central\n",
      "\tfound software.amazon.awssdk#json-utils;2.20.137 in central\n",
      "\tfound software.amazon.awssdk#third-party-jackson-core;2.20.137 in central\n",
      "\tfound software.amazon.awssdk#auth;2.20.137 in central\n",
      "\tfound software.amazon.eventstream#eventstream;1.0.1 in central\n",
      "\tfound software.amazon.awssdk#protocol-core;2.20.137 in central\n",
      "\tfound software.amazon.awssdk#apache-client;2.20.137 in central\n",
      "\tfound org.apache.httpcomponents#httpclient;4.5.13 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.4.13 in central\n",
      "\tfound commons-logging#commons-logging;1.2 in central\n",
      "\tfound commons-codec#commons-codec;1.15 in central\n",
      "\tfound software.amazon.awssdk#netty-nio-client;2.20.137 in central\n",
      "\tfound io.netty#netty-codec-http;4.1.94.Final in central\n",
      "\tfound io.netty#netty-common;4.1.94.Final in central\n",
      "\tfound io.netty#netty-buffer;4.1.94.Final in central\n",
      "\tfound io.netty#netty-transport;4.1.94.Final in central\n",
      "\tfound io.netty#netty-resolver;4.1.94.Final in central\n",
      "\tfound io.netty#netty-codec;4.1.94.Final in central\n",
      "\tfound io.netty#netty-handler;4.1.94.Final in central\n",
      "\tfound io.netty#netty-transport-native-unix-common;4.1.94.Final in central\n",
      "\tfound io.netty#netty-codec-http2;4.1.94.Final in central\n",
      "\tfound io.netty#netty-transport-classes-epoll;4.1.94.Final in central\n",
      "\tfound software.amazon.awssdk#s3;2.20.137 in central\n",
      "\tfound software.amazon.awssdk#aws-xml-protocol;2.20.137 in central\n",
      "\tfound software.amazon.awssdk#aws-query-protocol;2.20.137 in central\n",
      "\tfound software.amazon.awssdk#arns;2.20.137 in central\n",
      "\tfound software.amazon.awssdk#crt-core;2.20.137 in central\n",
      "\tfound software.amazon.awssdk#sts;2.20.137 in central\n",
      "\tfound software.amazon.awssdk#dynamodb;2.20.137 in central\n",
      "\tfound software.amazon.awssdk#kms;2.20.137 in central\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;10.3.0 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;4.8.2 in central\n",
      "\t[4.8.2] org.mongodb#mongodb-driver-sync;[4.8.1,4.8.99)\n",
      "\tfound org.mongodb#bson;4.8.2 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;4.8.2 in central\n",
      "\tfound org.mongodb#bson-record-codec;4.8.2 in central\n",
      "downloading https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-spark-runtime-3.5_2.12/1.5.2/iceberg-spark-runtime-3.5_2.12-1.5.2.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.5.2!iceberg-spark-runtime-3.5_2.12.jar (6190ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.6/hadoop-aws-3.3.6.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-aws;3.3.6!hadoop-aws.jar (171ms)\n",
      "downloading https://repo1.maven.org/maven2/software/amazon/awssdk/glue/2.20.137/glue-2.20.137.jar ...\n",
      "\t[SUCCESSFUL ] software.amazon.awssdk#glue;2.20.137!glue.jar (1363ms)\n",
      "downloading https://repo1.maven.org/maven2/software/amazon/awssdk/s3/2.20.137/s3-2.20.137.jar ...\n",
      "\t[SUCCESSFUL ] software.amazon.awssdk#s3;2.20.137!s3.jar (814ms)\n",
      "downloading https://repo1.maven.org/maven2/software/amazon/awssdk/sts/2.20.137/sts-2.20.137.jar ...\n",
      "\t[SUCCESSFUL ] software.amazon.awssdk#sts;2.20.137!sts.jar (350ms)\n",
      "downloading https://repo1.maven.org/maven2/software/amazon/awssdk/dynamodb/2.20.137/dynamodb-2.20.137.jar ...\n",
      "\t[SUCCESSFUL ] software.amazon.awssdk#dynamodb;2.20.137!dynamodb.jar (648ms)\n",
      "downloading https://repo1.maven.org/maven2/software/amazon/awssdk/kms/2.20.137/kms-2.20.137.jar ...\n",
      "\t[SUCCESSFUL ] software.amazon.awssdk#kms;2.20.137!kms.jar (390ms)\n",
      "downloading https://repo1.maven.org/maven2/org/mongodb/spark/mongo-spark-connector_2.12/10.3.0/mongo-spark-connector_2.12-10.3.0.jar ...\n",
      "\t[SUCCESSFUL ] org.mongodb.spark#mongo-spark-connector_2.12;10.3.0!mongo-spark-connector_2.12.jar (180ms)\n",
      "downloading https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.367/aws-java-sdk-bundle-1.12.367.jar ...\n",
      "\t[SUCCESSFUL ] com.amazonaws#aws-java-sdk-bundle;1.12.367!aws-java-sdk-bundle.jar (49023ms)\n",
      "downloading https://repo1.maven.org/maven2/org/wildfly/openssl/wildfly-openssl/1.1.3.Final/wildfly-openssl-1.1.3.Final.jar ...\n",
      "\t[SUCCESSFUL ] org.wildfly.openssl#wildfly-openssl;1.1.3.Final!wildfly-openssl.jar (175ms)\n",
      "downloading https://repo1.maven.org/maven2/software/amazon/awssdk/aws-json-protocol/2.20.137/aws-json-protocol-2.20.137.jar ...\n",
      "\t[SUCCESSFUL ] software.amazon.awssdk#aws-json-protocol;2.20.137!aws-json-protocol.jar (115ms)\n",
      "downloading https://repo1.maven.org/maven2/software/amazon/awssdk/protocol-core/2.20.137/protocol-core-2.20.137.jar ...\n",
      "\t[SUCCESSFUL ] software.amazon.awssdk#protocol-core;2.20.137!protocol-core.jar (61ms)\n",
      "downloading https://repo1.maven.org/maven2/software/amazon/awssdk/sdk-core/2.20.137/sdk-core-2.20.137.jar ...\n",
      "\t[SUCCESSFUL ] software.amazon.awssdk#sdk-core;2.20.137!sdk-core.jar (367ms)\n",
      "downloading https://repo1.maven.org/maven2/software/amazon/awssdk/auth/2.20.137/auth-2.20.137.jar ...\n",
      "\t[SUCCESSFUL ] software.amazon.awssdk#auth;2.20.137!auth.jar (95ms)\n",
      "downloading https://repo1.maven.org/maven2/software/amazon/awssdk/http-client-spi/2.20.137/http-client-spi-2.20.137.jar ...\n",
      "\t[SUCCESSFUL ] software.amazon.awssdk#http-client-spi;2.20.137!http-client-spi.jar (68ms)\n",
      "downloading https://repo1.maven.org/maven2/software/amazon/awssdk/regions/2.20.137/regions-2.20.137.jar ...\n",
      "\t[SUCCESSFUL ] software.amazon.awssdk#regions;2.20.137!regions.jar (187ms)\n",
      "downloading https://repo1.maven.org/maven2/software/amazon/awssdk/annotations/2.20.137/annotations-2.20.137.jar ...\n",
      "\t[SUCCESSFUL ] software.amazon.awssdk#annotations;2.20.137!annotations.jar (54ms)\n",
      "downloading https://repo1.maven.org/maven2/software/amazon/awssdk/utils/2.20.137/utils-2.20.137.jar ...\n",
      "\t[SUCCESSFUL ] software.amazon.awssdk#utils;2.20.137!utils.jar (94ms)\n",
      "downloading https://repo1.maven.org/maven2/software/amazon/awssdk/aws-core/2.20.137/aws-core-2.20.137.jar ...\n",
      "\t[SUCCESSFUL ] software.amazon.awssdk#aws-core;2.20.137!aws-core.jar (81ms)\n",
      "downloading https://repo1.maven.org/maven2/software/amazon/awssdk/metrics-spi/2.20.137/metrics-spi-2.20.137.jar ...\n",
      "\t[SUCCESSFUL ] software.amazon.awssdk#metrics-spi;2.20.137!metrics-spi.jar (68ms)\n",
      "downloading https://repo1.maven.org/maven2/software/amazon/awssdk/json-utils/2.20.137/json-utils-2.20.137.jar ...\n",
      "\t[SUCCESSFUL ] software.amazon.awssdk#json-utils;2.20.137!json-utils.jar (66ms)\n",
      "downloading https://repo1.maven.org/maven2/software/amazon/awssdk/endpoints-spi/2.20.137/endpoints-spi-2.20.137.jar ...\n",
      "\t[SUCCESSFUL ] software.amazon.awssdk#endpoints-spi;2.20.137!endpoints-spi.jar (53ms)\n",
      "downloading https://repo1.maven.org/maven2/software/amazon/awssdk/third-party-jackson-core/2.20.137/third-party-jackson-core-2.20.137.jar ...\n",
      "\t[SUCCESSFUL ] software.amazon.awssdk#third-party-jackson-core;2.20.137!third-party-jackson-core.jar (357ms)\n",
      "downloading https://repo1.maven.org/maven2/software/amazon/awssdk/profiles/2.20.137/profiles-2.20.137.jar ...\n",
      "\t[SUCCESSFUL ] software.amazon.awssdk#profiles;2.20.137!profiles.jar (64ms)\n",
      "downloading https://repo1.maven.org/maven2/software/amazon/eventstream/eventstream/1.0.1/eventstream-1.0.1.jar ...\n",
      "\t[SUCCESSFUL ] software.amazon.eventstream#eventstream;1.0.1!eventstream.jar (60ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.30/slf4j-api-1.7.30.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.30!slf4j-api.jar (62ms)\n",
      "downloading https://repo1.maven.org/maven2/org/reactivestreams/reactive-streams/1.0.4/reactive-streams-1.0.4.jar ...\n",
      "\t[SUCCESSFUL ] org.reactivestreams#reactive-streams;1.0.4!reactive-streams.jar (58ms)\n",
      "downloading https://repo1.maven.org/maven2/software/amazon/awssdk/apache-client/2.20.137/apache-client-2.20.137.jar ...\n",
      "\t[SUCCESSFUL ] software.amazon.awssdk#apache-client;2.20.137!apache-client.jar (85ms)\n",
      "downloading https://repo1.maven.org/maven2/software/amazon/awssdk/netty-nio-client/2.20.137/netty-nio-client-2.20.137.jar ...\n",
      "\t[SUCCESSFUL ] software.amazon.awssdk#netty-nio-client;2.20.137!netty-nio-client.jar (286ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/httpcomponents/httpclient/4.5.13/httpclient-4.5.13.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.httpcomponents#httpclient;4.5.13!httpclient.jar (273ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/httpcomponents/httpcore/4.4.13/httpcore-4.4.13.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.httpcomponents#httpcore;4.4.13!httpcore.jar (103ms)\n",
      "downloading https://repo1.maven.org/maven2/commons-codec/commons-codec/1.15/commons-codec-1.15.jar ...\n",
      "\t[SUCCESSFUL ] commons-codec#commons-codec;1.15!commons-codec.jar (340ms)\n",
      "downloading https://repo1.maven.org/maven2/commons-logging/commons-logging/1.2/commons-logging-1.2.jar ...\n",
      "\t[SUCCESSFUL ] commons-logging#commons-logging;1.2!commons-logging.jar (71ms)\n",
      "downloading https://repo1.maven.org/maven2/io/netty/netty-codec-http/4.1.94.Final/netty-codec-http-4.1.94.Final.jar ...\n",
      "\t[SUCCESSFUL ] io.netty#netty-codec-http;4.1.94.Final!netty-codec-http.jar (121ms)\n",
      "downloading https://repo1.maven.org/maven2/io/netty/netty-codec-http2/4.1.94.Final/netty-codec-http2-4.1.94.Final.jar ...\n",
      "\t[SUCCESSFUL ] io.netty#netty-codec-http2;4.1.94.Final!netty-codec-http2.jar (189ms)\n",
      "downloading https://repo1.maven.org/maven2/io/netty/netty-codec/4.1.94.Final/netty-codec-4.1.94.Final.jar ...\n",
      "\t[SUCCESSFUL ] io.netty#netty-codec;4.1.94.Final!netty-codec.jar (112ms)\n",
      "downloading https://repo1.maven.org/maven2/io/netty/netty-transport/4.1.94.Final/netty-transport-4.1.94.Final.jar ...\n",
      "\t[SUCCESSFUL ] io.netty#netty-transport;4.1.94.Final!netty-transport.jar (235ms)\n",
      "downloading https://repo1.maven.org/maven2/io/netty/netty-common/4.1.94.Final/netty-common-4.1.94.Final.jar ...\n",
      "\t[SUCCESSFUL ] io.netty#netty-common;4.1.94.Final!netty-common.jar (218ms)\n",
      "downloading https://repo1.maven.org/maven2/io/netty/netty-buffer/4.1.94.Final/netty-buffer-4.1.94.Final.jar ...\n",
      "\t[SUCCESSFUL ] io.netty#netty-buffer;4.1.94.Final!netty-buffer.jar (194ms)\n",
      "downloading https://repo1.maven.org/maven2/io/netty/netty-handler/4.1.94.Final/netty-handler-4.1.94.Final.jar ...\n",
      "\t[SUCCESSFUL ] io.netty#netty-handler;4.1.94.Final!netty-handler.jar (156ms)\n",
      "downloading https://repo1.maven.org/maven2/io/netty/netty-transport-classes-epoll/4.1.94.Final/netty-transport-classes-epoll-4.1.94.Final.jar ...\n",
      "\t[SUCCESSFUL ] io.netty#netty-transport-classes-epoll;4.1.94.Final!netty-transport-classes-epoll.jar (138ms)\n",
      "downloading https://repo1.maven.org/maven2/io/netty/netty-resolver/4.1.94.Final/netty-resolver-4.1.94.Final.jar ...\n",
      "\t[SUCCESSFUL ] io.netty#netty-resolver;4.1.94.Final!netty-resolver.jar (61ms)\n",
      "downloading https://repo1.maven.org/maven2/io/netty/netty-transport-native-unix-common/4.1.94.Final/netty-transport-native-unix-common-4.1.94.Final.jar ...\n",
      "\t[SUCCESSFUL ] io.netty#netty-transport-native-unix-common;4.1.94.Final!netty-transport-native-unix-common.jar (71ms)\n",
      "downloading https://repo1.maven.org/maven2/software/amazon/awssdk/aws-xml-protocol/2.20.137/aws-xml-protocol-2.20.137.jar ...\n",
      "\t[SUCCESSFUL ] software.amazon.awssdk#aws-xml-protocol;2.20.137!aws-xml-protocol.jar (104ms)\n",
      "downloading https://repo1.maven.org/maven2/software/amazon/awssdk/arns/2.20.137/arns-2.20.137.jar ...\n",
      "\t[SUCCESSFUL ] software.amazon.awssdk#arns;2.20.137!arns.jar (61ms)\n",
      "downloading https://repo1.maven.org/maven2/software/amazon/awssdk/crt-core/2.20.137/crt-core-2.20.137.jar ...\n",
      "\t[SUCCESSFUL ] software.amazon.awssdk#crt-core;2.20.137!crt-core.jar (63ms)\n",
      "downloading https://repo1.maven.org/maven2/software/amazon/awssdk/aws-query-protocol/2.20.137/aws-query-protocol-2.20.137.jar ...\n",
      "\t[SUCCESSFUL ] software.amazon.awssdk#aws-query-protocol;2.20.137!aws-query-protocol.jar (75ms)\n",
      "downloading https://repo1.maven.org/maven2/org/mongodb/mongodb-driver-sync/4.8.2/mongodb-driver-sync-4.8.2.jar ...\n",
      "\t[SUCCESSFUL ] org.mongodb#mongodb-driver-sync;4.8.2!mongodb-driver-sync.jar (96ms)\n",
      "downloading https://repo1.maven.org/maven2/org/mongodb/bson/4.8.2/bson-4.8.2.jar ...\n",
      "\t[SUCCESSFUL ] org.mongodb#bson;4.8.2!bson.jar (310ms)\n",
      "downloading https://repo1.maven.org/maven2/org/mongodb/mongodb-driver-core/4.8.2/mongodb-driver-core-4.8.2.jar ...\n",
      "\t[SUCCESSFUL ] org.mongodb#mongodb-driver-core;4.8.2!mongodb-driver-core.jar (337ms)\n",
      "downloading https://repo1.maven.org/maven2/org/mongodb/bson-record-codec/4.8.2/bson-record-codec-4.8.2.jar ...\n",
      "\t[SUCCESSFUL ] org.mongodb#bson-record-codec;4.8.2!bson-record-codec.jar (65ms)\n",
      ":: resolution report :: resolve 11077ms :: artifacts dl 65018ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.367 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.15 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.2 from central in [default]\n",
      "\tio.netty#netty-buffer;4.1.94.Final from central in [default]\n",
      "\tio.netty#netty-codec;4.1.94.Final from central in [default]\n",
      "\tio.netty#netty-codec-http;4.1.94.Final from central in [default]\n",
      "\tio.netty#netty-codec-http2;4.1.94.Final from central in [default]\n",
      "\tio.netty#netty-common;4.1.94.Final from central in [default]\n",
      "\tio.netty#netty-handler;4.1.94.Final from central in [default]\n",
      "\tio.netty#netty-resolver;4.1.94.Final from central in [default]\n",
      "\tio.netty#netty-transport;4.1.94.Final from central in [default]\n",
      "\tio.netty#netty-transport-classes-epoll;4.1.94.Final from central in [default]\n",
      "\tio.netty#netty-transport-native-unix-common;4.1.94.Final from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.6 from central in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.5.13 from central in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.4.13 from central in [default]\n",
      "\torg.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.5.2 from central in [default]\n",
      "\torg.mongodb#bson;4.8.2 from central in [default]\n",
      "\torg.mongodb#bson-record-codec;4.8.2 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;4.8.2 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;4.8.2 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;10.3.0 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.4 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.1.3.Final from central in [default]\n",
      "\tsoftware.amazon.awssdk#annotations;2.20.137 from central in [default]\n",
      "\tsoftware.amazon.awssdk#apache-client;2.20.137 from central in [default]\n",
      "\tsoftware.amazon.awssdk#arns;2.20.137 from central in [default]\n",
      "\tsoftware.amazon.awssdk#auth;2.20.137 from central in [default]\n",
      "\tsoftware.amazon.awssdk#aws-core;2.20.137 from central in [default]\n",
      "\tsoftware.amazon.awssdk#aws-json-protocol;2.20.137 from central in [default]\n",
      "\tsoftware.amazon.awssdk#aws-query-protocol;2.20.137 from central in [default]\n",
      "\tsoftware.amazon.awssdk#aws-xml-protocol;2.20.137 from central in [default]\n",
      "\tsoftware.amazon.awssdk#crt-core;2.20.137 from central in [default]\n",
      "\tsoftware.amazon.awssdk#dynamodb;2.20.137 from central in [default]\n",
      "\tsoftware.amazon.awssdk#endpoints-spi;2.20.137 from central in [default]\n",
      "\tsoftware.amazon.awssdk#glue;2.20.137 from central in [default]\n",
      "\tsoftware.amazon.awssdk#http-client-spi;2.20.137 from central in [default]\n",
      "\tsoftware.amazon.awssdk#json-utils;2.20.137 from central in [default]\n",
      "\tsoftware.amazon.awssdk#kms;2.20.137 from central in [default]\n",
      "\tsoftware.amazon.awssdk#metrics-spi;2.20.137 from central in [default]\n",
      "\tsoftware.amazon.awssdk#netty-nio-client;2.20.137 from central in [default]\n",
      "\tsoftware.amazon.awssdk#profiles;2.20.137 from central in [default]\n",
      "\tsoftware.amazon.awssdk#protocol-core;2.20.137 from central in [default]\n",
      "\tsoftware.amazon.awssdk#regions;2.20.137 from central in [default]\n",
      "\tsoftware.amazon.awssdk#s3;2.20.137 from central in [default]\n",
      "\tsoftware.amazon.awssdk#sdk-core;2.20.137 from central in [default]\n",
      "\tsoftware.amazon.awssdk#sts;2.20.137 from central in [default]\n",
      "\tsoftware.amazon.awssdk#third-party-jackson-core;2.20.137 from central in [default]\n",
      "\tsoftware.amazon.awssdk#utils;2.20.137 from central in [default]\n",
      "\tsoftware.amazon.eventstream#eventstream;1.0.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   51  |   51  |   51  |   0   ||   51  |   51  |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-9939de60-eec6-466c-9379-ef2563aa3003\n",
      "\tconfs: [default]\n",
      "\t51 artifacts copied, 0 already retrieved (370087kB/3703ms)\n",
      "25/09/23 17:31:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|createtab_stmt                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|CREATE TABLE iceberg.raw.ecommerce_db_orders (\\n  _id STRING,\\n  order_id STRING,\\n  customer_id STRING,\\n  order_date STRING,\\n  status STRING,\\n  total_amount STRING,\\n  lines ARRAY<STRUCT<product_id: STRING, quantity: INT, unit_price: STRING, line_total: STRING>>,\\n  ingest_ts TIMESTAMP)\\nUSING iceberg\\nLOCATION 's3a://fq-app-analytics-bucket-1/iceberg-warehouse/raw.db/ecommerce_db_orders'\\nTBLPROPERTIES (\\n  'current-snapshot-id' = '1477696781060725643',\\n  'format' = 'iceberg/parquet',\\n  'format-version' = '2',\\n  'write.parquet.compression-codec' = 'zstd')\\n|\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = 'AKIA5QMIH4RHVEXR3P4F'\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = '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'\n",
    "os.environ['AWS_REGION'] = 'us-east-2'\n",
    "os.environ['S3_BUCKET'] ='fq-app-analytics-bucket-1'\n",
    "\n",
    "PACKAGES = \",\".join([\n",
    "    \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.2\",\n",
    "    \"org.apache.hadoop:hadoop-aws:3.3.6\",\n",
    "    # AWS SDK v2 clients required when using GlueCatalog\n",
    "    \"software.amazon.awssdk:glue:2.20.137\",\n",
    "    \"software.amazon.awssdk:s3:2.20.137\",\n",
    "    \"software.amazon.awssdk:sts:2.20.137\",\n",
    "    \"software.amazon.awssdk:dynamodb:2.20.137\",\n",
    "    \"software.amazon.awssdk:kms:2.20.137\",\n",
    "    # add mongo connector only if needed in this notebook\n",
    "    \"org.mongodb.spark:mongo-spark-connector_2.12:10.3.0\"\n",
    "])\n",
    "\n",
    "GLUE_CATALOG = \"iceberg\"   # the logical catalog name you used in ETL\n",
    "WAREHOUSE = \"s3a://fq-app-analytics-bucket-1/iceberg-warehouse/\"\n",
    "AWS_REGION = os.environ.get(\"AWS_REGION\", os.environ.get(\"AWS_DEFAULT_REGION\", \"us-east-1\"))\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"notebook-iceberg-debug\")\n",
    "    .config(\"spark.jars.packages\", PACKAGES)\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\n",
    "    .config(f\"spark.sql.catalog.{GLUE_CATALOG}\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    .config(f\"spark.sql.catalog.{GLUE_CATALOG}.catalog-impl\", \"org.apache.iceberg.aws.glue.GlueCatalog\")\n",
    "    .config(f\"spark.sql.catalog.{GLUE_CATALOG}.warehouse\", WAREHOUSE)\n",
    "    # make sure SDK sees region\n",
    "    .config(\"spark.hadoop.fs.s3a.region\", AWS_REGION)\n",
    "    .config(\"spark.driver.extraJavaOptions\", f\"-Daws.region={AWS_REGION}\")\n",
    "    .config(\"spark.executor.extraJavaOptions\", f\"-Daws.region={AWS_REGION}\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# run the SHOW CREATE TABLE statement\n",
    "spark.sql(\"SHOW CREATE TABLE iceberg.raw.ecommerce_db_orders\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2b822ff-bb8d-4223-990f-20fc5dbb090e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Catalog impl: org.apache.iceberg.spark.SparkCatalog\n",
      "Warehouse: s3a://fq-app-analytics-bucket-1/iceberg-warehouse/\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|createtab_stmt                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|CREATE TABLE iceberg.raw.ecommerce_db_orders (\\n  _id STRING,\\n  order_id STRING,\\n  customer_id STRING,\\n  order_date STRING,\\n  status STRING,\\n  total_amount STRING,\\n  lines ARRAY<STRUCT<product_id: STRING, quantity: INT, unit_price: STRING, line_total: STRING>>,\\n  ingest_ts TIMESTAMP)\\nUSING iceberg\\nLOCATION 's3a://fq-app-analytics-bucket-1/iceberg-warehouse/raw.db/ecommerce_db_orders'\\nTBLPROPERTIES (\\n  'current-snapshot-id' = '1477696781060725643',\\n  'format' = 'iceberg/parquet',\\n  'format-version' = '2',\\n  'write.parquet.compression-codec' = 'zstd')\\n|\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verify catalog config\n",
    "print(\"Catalog impl:\", spark.conf.get(f\"spark.sql.catalog.{GLUE_CATALOG}\"))\n",
    "print(\"Warehouse:\", spark.conf.get(f\"spark.sql.catalog.{GLUE_CATALOG}.warehouse\"))\n",
    "\n",
    "# Now run the SHOW CREATE TABLE\n",
    "spark.sql(\"SHOW CREATE TABLE iceberg.raw.ecommerce_db_orders\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55e03d4-4d7a-4d68-95ab-709d45bf9546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Initialize Spark with Iceberg and AWS Glue Catalog\n",
    "import os\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Set environment variables (if not already set in container)\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "GLUE_CATALOG = os.environ.get(\"GLUE_CATALOG_NAME\", \"iceberg\")   # the logical catalog name you used in ETL\n",
    "WAREHOUSE = os.environ.get(\"WAREHOUSE\", \"s3a://fq-app-analytics-bucket-1/iceberg-warehouse/\") \n",
    "AWS_REGION = os.environ.get(\"AWS_REGION\", os.environ.get(\"AWS_DEFAULT_REGION\", \"us-east-2\"))\n",
    "RAW_TABLE = os.environ.get(\"RAW_TABLE\", \"raw.ecommerce_db_orders\")  # catalog-qualified without prefix\n",
    "CURATED_TABLE = os.environ.get(\"CURATED_TABLE\", \"curated.ecommerce_db_orders_silver\")\n",
    "\n",
    "# Define packages\n",
    "PACKAGES = \",\".join([\n",
    "    \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.2\",\n",
    "    \"org.apache.hadoop:hadoop-aws:3.3.6\",\n",
    "    # AWS SDK v2 clients required when using GlueCatalog\n",
    "    \"software.amazon.awssdk:glue:2.20.137\",\n",
    "    \"software.amazon.awssdk:s3:2.20.137\", \n",
    "    \"software.amazon.awssdk:sts:2.20.137\",\n",
    "    \"software.amazon.awssdk:dynamodb:2.20.137\",\n",
    "    \"software.amazon.awssdk:kms:2.20.137\",\n",
    "    \"org.mongodb.spark:mongo-spark-connector_2.12:10.3.0\"\n",
    "])\n",
    "\n",
    "# Create Spark session with Iceberg and Glue Catalog\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Jupyter-Iceberg-Analytics\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.jars.packages\", PACKAGES) \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .config(f\"spark.sql.catalog.{GLUE_CATALOG}\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(f\"spark.sql.catalog.{GLUE_CATALOG}.catalog-impl\", \"org.apache.iceberg.aws.glue.GlueCatalog\") \\\n",
    "    .config(f\"spark.sql.catalog.{GLUE_CATALOG}.type\", \"glue\") \\\n",
    "    .config(f\"spark.sql.catalog.{GLUE_CATALOG}.warehouse\", f\"s3://{os.getenv('S3_BUCKET')}/iceberg-warehouse\") \\\n",
    "    .config(f\"spark.sql.catalog.{GLUE_CATALOG}.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\") \\\n",
    "    .config(\"spark.sql.defaultCatalog\", \"iceberg\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", os.getenv('AWS_ACCESS_KEY_ID')) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", os.getenv('AWS_SECRET_ACCESS_KEY')) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", f\"s3.{os.getenv('AWS_DEFAULT_REGION', 'us-east-1')}.amazonaws.com\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"false\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.multipart.size\", \"104857600\") \\\n",
    "    .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "    # builder = builder \\\n",
    "    #     .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    #     .config(f\"spark.sql.catalog.{GLUE_CATALOG_NAME}\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    #     .config(f\"spark.sql.catalog.{GLUE_CATALOG_NAME}.catalog-impl\", \"org.apache.iceberg.aws.glue.GlueCatalog\") \\\n",
    "    #     .config(f\"spark.sql.catalog.{GLUE_CATALOG_NAME}.warehouse\", WAREHOUSE) \\\n",
    "    #     .config(\"spark.sql.catalogImplementation\", \"hive\")  # ensure Hive-style catalog support\n",
    "    # return builder.getOrCreate()\n",
    "\n",
    "# Set log level to reduce noise\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(f\"✅ Spark {spark.version} initialized successfully!\")\n",
    "print(f\"🗄️  Default catalog: {spark.conf.get('spark.sql.defaultCatalog')}\")\n",
    "print(f\"🌐 Spark UI: http://localhost:4040\")\n",
    "\n",
    "# run the SHOW CREATE TABLE statement\n",
    "spark.sql(\"SHOW CREATE TABLE iceberg.raw.ecommerce_db_orders\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a63499a-f988-4b6e-98cd-9d8acb3df403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Available Databases ===\n",
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "+---------+\n",
      "\n",
      "\n",
      "=== Available Tables in ecommerce database ===\n",
      "Database 'ecommerce' might not exist yet: Nested databases are not supported by v1 session catalog: glue_catalog.ecommerce.\n",
      "Available databases:\n",
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "+---------+\n",
      "\n",
      "\n",
      "=== Spark Catalogs ===\n",
      "+-------------+\n",
      "|      catalog|\n",
      "+-------------+\n",
      "|      iceberg|\n",
      "|spark_catalog|\n",
      "+-------------+\n",
      "\n",
      "\n",
      "=== Environment Check ===\n",
      "S3 Bucket: fq-app-analytics-bucket-1\n",
      "AWS Region: None\n",
      "AWS Access Key: AKIA5QMIH4...\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Verify setup and explore available data\n",
    "print(\"=== Available Databases ===\")\n",
    "spark.sql(\"SHOW DATABASES\").show()\n",
    "\n",
    "print(\"\\n=== Available Tables in ecommerce database ===\")\n",
    "try:\n",
    "    spark.sql(\"SHOW TABLES IN glue_catalog.ecommerce\").show()\n",
    "except Exception as e:\n",
    "    print(f\"Database 'ecommerce' might not exist yet: {e}\")\n",
    "    print(\"Available databases:\")\n",
    "    spark.sql(\"SHOW DATABASES\").show()\n",
    "\n",
    "print(\"\\n=== Spark Catalogs ===\")\n",
    "spark.sql(\"SHOW CATALOGS\").show()\n",
    "\n",
    "print(\"\\n=== Environment Check ===\")\n",
    "print(f\"S3 Bucket: {os.getenv('S3_BUCKET')}\")\n",
    "print(f\"AWS Region: {os.getenv('AWS_DEFAULT_REGION')}\")\n",
    "print(f\"AWS Access Key: {os.getenv('AWS_ACCESS_KEY_ID', 'NOT_SET')[:10]}...\" if os.getenv('AWS_ACCESS_KEY_ID') else \"NOT_SET\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c978a019-9dcd-42dd-bb35-572522f07ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available databases:iceberg.raw.ecommerce_db_orders\n",
      "Available databases:iceberg.curated.fq_app_orders_silver\n",
      "+------------------------+----------+-----------+--------------------------+---------+------------+-----+--------------------------+\n",
      "|_id                     |order_id  |customer_id|order_date                |status   |total_amount|lines|ingest_ts                 |\n",
      "+------------------------+----------+-----------+--------------------------+---------+------------+-----+--------------------------+\n",
      "|68cc35a49c272db26b10a13b|ORD-000010|NULL       |2024-11-12T12:38:59.997000|cancelled|NULL        |NULL |2025-09-23 15:20:58.646635|\n",
      "|68cc35a49c272db26b10a153|ORD-000034|NULL       |2025-04-14T12:38:59.997000|completed|NULL        |NULL |2025-09-23 15:20:58.646635|\n",
      "|68cc35a49c272db26b10a157|ORD-000038|NULL       |2025-01-19T12:38:59.997000|pending  |NULL        |NULL |2025-09-23 15:20:58.646635|\n",
      "|68cc35a49c272db26b10a158|ORD-000039|NULL       |2025-01-19T12:38:59.997000|pending  |NULL        |NULL |2025-09-23 15:20:58.646635|\n",
      "|68cc35a49c272db26b10a164|ORD-000051|NULL       |2024-12-14T12:38:59.998000|pending  |NULL        |NULL |2025-09-23 15:20:58.646635|\n",
      "|68cc35a49c272db26b10a165|ORD-000052|NULL       |2024-11-25T12:38:59.998000|cancelled|NULL        |NULL |2025-09-23 15:20:58.646635|\n",
      "|68cc35a49c272db26b10a16f|ORD-000062|NULL       |2025-09-13T12:38:59.998000|pending  |NULL        |NULL |2025-09-23 15:20:58.646635|\n",
      "|68cc35a49c272db26b10a173|ORD-000066|NULL       |2024-11-24T12:38:59.998000|cancelled|NULL        |NULL |2025-09-23 15:20:58.646635|\n",
      "|68cc35a49c272db26b10a17c|ORD-000075|NULL       |2025-07-08T12:38:59.998000|pending  |NULL        |NULL |2025-09-23 15:20:58.646635|\n",
      "|68cc35a49c272db26b10a1bd|ORD-000140|NULL       |2025-07-07T12:38:59.998000|completed|NULL        |NULL |2025-09-23 15:20:58.646635|\n",
      "|68cc35a49c272db26b10a1f3|ORD-000194|NULL       |2024-12-05T12:38:59.998000|completed|NULL        |NULL |2025-09-23 15:20:58.646635|\n",
      "|68cc35a49c272db26b10a1f4|ORD-000195|NULL       |2025-06-13T12:38:59.998000|pending  |NULL        |NULL |2025-09-23 15:20:58.646635|\n",
      "|68cc35a49c272db26b10a219|ORD-000232|NULL       |2025-08-17T12:38:59.998000|pending  |NULL        |NULL |2025-09-23 15:20:58.646635|\n",
      "|68cc35a49c272db26b10a23b|ORD-000266|NULL       |2025-01-23T12:38:59.999000|cancelled|NULL        |NULL |2025-09-23 15:20:58.646635|\n",
      "|68cc35a49c272db26b10a23c|ORD-000267|NULL       |2025-05-27T12:38:59.999000|cancelled|NULL        |NULL |2025-09-23 15:20:58.646635|\n",
      "|68cc35a49c272db26b10a249|ORD-000280|NULL       |2025-03-29T12:38:59.999000|pending  |NULL        |NULL |2025-09-23 15:20:58.646635|\n",
      "|68cc35a49c272db26b10a255|ORD-000292|NULL       |2025-09-14T12:38:59.999000|cancelled|NULL        |NULL |2025-09-23 15:20:58.646635|\n",
      "|68cc35a49c272db26b10a285|ORD-000340|NULL       |2024-10-07T12:38:59.999000|cancelled|NULL        |NULL |2025-09-23 15:20:58.646635|\n",
      "|68cc35a49c272db26b10a288|ORD-000343|NULL       |2024-09-18T12:38:59.999000|pending  |NULL        |NULL |2025-09-23 15:20:58.646635|\n",
      "|68cc35a49c272db26b10a2a6|ORD-000373|NULL       |2025-09-08T12:38:59.999000|pending  |NULL        |NULL |2025-09-23 15:20:58.646635|\n",
      "+------------------------+----------+-----------+--------------------------+---------+------------+-----+--------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/23 17:49:51 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+----------+-----------+--------------------+---------+------------+\n",
      "|summary|                 _id|  order_id|customer_id|          order_date|   status|total_amount|\n",
      "+-------+--------------------+----------+-----------+--------------------+---------+------------+\n",
      "|  count|                2000|      2000|          0|                2000|     2000|           0|\n",
      "|   mean|                NULL|      NULL|       NULL|                NULL|     NULL|        NULL|\n",
      "| stddev|                NULL|      NULL|       NULL|                NULL|     NULL|        NULL|\n",
      "|    min|68cc35a49c272db26...|ORD-000001|       NULL|2024-09-18T12:38:...|cancelled|        NULL|\n",
      "|    max|68cc35a49c272db26...|ORD-001000|       NULL|2025-09-18T12:39:...|  pending|        NULL|\n",
      "+-------+--------------------+----------+-----------+--------------------+---------+------------+\n",
      "\n",
      "+------------------------+----------+-----------+----------+---------+------------+-----+-------------------------+\n",
      "|_id                     |order_id  |customer_id|order_date|status   |total_amount|lines|ingest_ts                |\n",
      "+------------------------+----------+-----------+----------+---------+------------+-----+-------------------------+\n",
      "|68cc35a49c272db26b10a13b|ORD-000010|NULL       |2024-11-12|cancelled|NULL        |NULL |2025-09-23 13:55:41.95841|\n",
      "|68cc35a49c272db26b10a153|ORD-000034|NULL       |2025-04-14|completed|NULL        |NULL |2025-09-23 13:55:41.95841|\n",
      "|68cc35a49c272db26b10a157|ORD-000038|NULL       |2025-01-19|pending  |NULL        |NULL |2025-09-23 13:55:41.95841|\n",
      "|68cc35a49c272db26b10a158|ORD-000039|NULL       |2025-01-19|pending  |NULL        |NULL |2025-09-23 13:55:41.95841|\n",
      "|68cc35a49c272db26b10a164|ORD-000051|NULL       |2024-12-14|pending  |NULL        |NULL |2025-09-23 13:55:41.95841|\n",
      "|68cc35a49c272db26b10a165|ORD-000052|NULL       |2024-11-25|cancelled|NULL        |NULL |2025-09-23 13:55:41.95841|\n",
      "|68cc35a49c272db26b10a16f|ORD-000062|NULL       |2025-09-13|pending  |NULL        |NULL |2025-09-23 13:55:41.95841|\n",
      "|68cc35a49c272db26b10a173|ORD-000066|NULL       |2024-11-24|cancelled|NULL        |NULL |2025-09-23 13:55:41.95841|\n",
      "|68cc35a49c272db26b10a17c|ORD-000075|NULL       |2025-07-08|pending  |NULL        |NULL |2025-09-23 13:55:41.95841|\n",
      "|68cc35a49c272db26b10a1bd|ORD-000140|NULL       |2025-07-07|completed|NULL        |NULL |2025-09-23 13:55:41.95841|\n",
      "|68cc35a49c272db26b10a1f3|ORD-000194|NULL       |2024-12-05|completed|NULL        |NULL |2025-09-23 13:55:41.95841|\n",
      "|68cc35a49c272db26b10a1f4|ORD-000195|NULL       |2025-06-13|pending  |NULL        |NULL |2025-09-23 13:55:41.95841|\n",
      "|68cc35a49c272db26b10a219|ORD-000232|NULL       |2025-08-17|pending  |NULL        |NULL |2025-09-23 13:55:41.95841|\n",
      "|68cc35a49c272db26b10a23b|ORD-000266|NULL       |2025-01-23|cancelled|NULL        |NULL |2025-09-23 13:55:41.95841|\n",
      "|68cc35a49c272db26b10a23c|ORD-000267|NULL       |2025-05-27|cancelled|NULL        |NULL |2025-09-23 13:55:41.95841|\n",
      "|68cc35a49c272db26b10a249|ORD-000280|NULL       |2025-03-29|pending  |NULL        |NULL |2025-09-23 13:55:41.95841|\n",
      "|68cc35a49c272db26b10a255|ORD-000292|NULL       |2025-09-14|cancelled|NULL        |NULL |2025-09-23 13:55:41.95841|\n",
      "|68cc35a49c272db26b10a285|ORD-000340|NULL       |2024-10-07|cancelled|NULL        |NULL |2025-09-23 13:55:41.95841|\n",
      "|68cc35a49c272db26b10a288|ORD-000343|NULL       |2024-09-18|pending  |NULL        |NULL |2025-09-23 13:55:41.95841|\n",
      "|68cc35a49c272db26b10a2a6|ORD-000373|NULL       |2025-09-08|pending  |NULL        |NULL |2025-09-23 13:55:41.95841|\n",
      "+------------------------+----------+-----------+----------+---------+------------+-----+-------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+----------+-----------+---------+------------+\n",
      "|summary|                 _id|  order_id|customer_id|   status|total_amount|\n",
      "+-------+--------------------+----------+-----------+---------+------------+\n",
      "|  count|                2000|      2000|          0|     2000|           0|\n",
      "|   mean|                NULL|      NULL|       NULL|     NULL|        NULL|\n",
      "| stddev|                NULL|      NULL|       NULL|     NULL|        NULL|\n",
      "|    min|68cc35a49c272db26...|ORD-000001|       NULL|cancelled|        NULL|\n",
      "|    max|68cc35a49c272db26...|ORD-001000|       NULL|  pending|        NULL|\n",
      "+-------+--------------------+----------+-----------+---------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+----------+-----------+----------+---------+------------+-----+--------------------------+\n",
      "|_id                     |order_id  |customer_id|order_date|status   |total_amount|lines|ingest_ts                 |\n",
      "+------------------------+----------+-----------+----------+---------+------------+-----+--------------------------+\n",
      "|68cc35a49c272db26b10a132|ORD-000001|NULL       |2025-07-09|completed|NULL        |NULL |2025-09-23 15:20:58.646635|\n",
      "|68cc35a49c272db26b10a133|ORD-000002|NULL       |2025-01-14|completed|NULL        |NULL |2025-09-23 15:20:58.646635|\n",
      "|68cc35a49c272db26b10a134|ORD-000003|NULL       |2025-05-07|cancelled|NULL        |NULL |2025-09-23 15:20:58.646635|\n",
      "|68cc35a49c272db26b10a135|ORD-000004|NULL       |2024-12-31|cancelled|NULL        |NULL |2025-09-23 15:20:58.646635|\n",
      "|68cc35a49c272db26b10a136|ORD-000005|NULL       |2024-11-24|completed|NULL        |NULL |2025-09-23 15:20:58.646635|\n",
      "|68cc35a49c272db26b10a137|ORD-000006|NULL       |2025-02-26|cancelled|NULL        |NULL |2025-09-23 15:20:58.646635|\n",
      "|68cc35a49c272db26b10a138|ORD-000007|NULL       |2025-04-02|pending  |NULL        |NULL |2025-09-23 15:20:58.646635|\n",
      "|68cc35a49c272db26b10a139|ORD-000008|NULL       |2025-06-16|completed|NULL        |NULL |2025-09-23 15:20:58.646635|\n",
      "|68cc35a49c272db26b10a13a|ORD-000009|NULL       |2025-06-28|cancelled|NULL        |NULL |2025-09-23 15:20:58.646635|\n",
      "|68cc35a49c272db26b10a13b|ORD-000010|NULL       |2024-11-12|cancelled|NULL        |NULL |2025-09-23 15:20:58.646635|\n",
      "|68cc35a49c272db26b10a13c|ORD-000011|NULL       |2025-09-10|cancelled|NULL        |NULL |2025-09-23 15:20:58.646635|\n",
      "|68cc35a49c272db26b10a13d|ORD-000012|NULL       |2025-01-05|pending  |NULL        |NULL |2025-09-23 15:20:58.646635|\n",
      "|68cc35a49c272db26b10a13e|ORD-000013|NULL       |2024-12-07|completed|NULL        |NULL |2025-09-23 15:20:58.646635|\n",
      "|68cc35a49c272db26b10a13f|ORD-000014|NULL       |2024-11-14|completed|NULL        |NULL |2025-09-23 15:20:58.646635|\n",
      "|68cc35a49c272db26b10a140|ORD-000015|NULL       |2024-09-29|pending  |NULL        |NULL |2025-09-23 15:20:58.646635|\n",
      "|68cc35a49c272db26b10a141|ORD-000016|NULL       |2025-08-26|cancelled|NULL        |NULL |2025-09-23 15:20:58.646635|\n",
      "|68cc35a49c272db26b10a142|ORD-000017|NULL       |2025-04-21|pending  |NULL        |NULL |2025-09-23 15:20:58.646635|\n",
      "|68cc35a49c272db26b10a143|ORD-000018|NULL       |2025-01-31|completed|NULL        |NULL |2025-09-23 15:20:58.646635|\n",
      "|68cc35a49c272db26b10a144|ORD-000019|NULL       |2025-05-02|pending  |NULL        |NULL |2025-09-23 15:20:58.646635|\n",
      "|68cc35a49c272db26b10a145|ORD-000020|NULL       |2024-11-16|cancelled|NULL        |NULL |2025-09-23 15:20:58.646635|\n",
      "+------------------------+----------+-----------+----------+---------+------------+-----+--------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 19:>                                                         (0 + 6) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+----------+-----------+---------+------------+\n",
      "|summary|                 _id|  order_id|customer_id|   status|total_amount|\n",
      "+-------+--------------------+----------+-----------+---------+------------+\n",
      "|  count|                1000|      1000|          0|     1000|           0|\n",
      "|   mean|                NULL|      NULL|       NULL|     NULL|        NULL|\n",
      "| stddev|                NULL|      NULL|       NULL|     NULL|        NULL|\n",
      "|    min|68cc35a49c272db26...|ORD-000001|       NULL|cancelled|        NULL|\n",
      "|    max|68cc35a49c272db26...|ORD-001000|       NULL|  pending|        NULL|\n",
      "+-------+--------------------+----------+-----------+---------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, explode, sum as spark_sum, count as spark_count,\n",
    "    countDistinct, to_date, coalesce, lit, expr, current_timestamp, max as spark_max\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "GLUE_CATALOG = os.environ.get(\"GLUE_CATALOG_NAME\", \"iceberg\")   # same catalog used in ETL\n",
    "RAW_TABLE = os.environ.get(\"RAW_TABLE\", \"raw.ecommerce_db_orders\")  # catalog-qualified without prefix\n",
    "CURATED_TABLE = os.environ.get(\"CURATED_TABLE\", \"curated.fq_app_orders_silver\")\n",
    "WAREHOUSE = os.environ.get(\"WAREHOUSE\", \"s3a://fq-app-analytics-bucket-1/iceberg-warehouse/\")  # same as ETL\n",
    "AWS_REGION = os.environ.get(\"AWS_REGION\", os.environ.get(\"AWS_DEFAULT_REGION\", \"us-east-2\"))\n",
    "\n",
    "RAW_QUALIFIED = f\"{GLUE_CATALOG}.{RAW_TABLE}\"\n",
    "CURATED_QUALIFIED = f\"{GLUE_CATALOG}.{CURATED_TABLE}\"\n",
    "\n",
    "print(f\"Available databases:{RAW_QUALIFIED}\")\n",
    "print(f\"Available databases:{CURATED_QUALIFIED}\")\n",
    "\n",
    "# db_name = CURATED_TABLE.split(\".\", 1)[0]  # e.g. 'curated'\n",
    "# qualified_db = f\"{GLUE_CATALOG}.{db_name}\"\n",
    "# try:\n",
    "#     spark.sql(f\"CREATE DATABASE IF NOT EXISTS {qualified_db}\")\n",
    "# except Exception as e:\n",
    "#     # fallback to namespace\n",
    "#     spark.sql(f\"CREATE NAMESPACE IF NOT EXISTS {qualified_db}\")\n",
    "\n",
    "raw_df = spark.table(RAW_QUALIFIED)\n",
    "\n",
    "raw_df.show(truncate=False)\n",
    "\n",
    "raw_df.describe().show()\n",
    "\n",
    "    # normalize fields:\n",
    "    # - ensure order_date is date type (raw may have string)\n",
    "    # - ensure ingest_ts exists\n",
    "df = raw_df.withColumn(\"order_date\", to_date(col(\"order_date\"))) \\\n",
    "           .withColumn(\"ingest_ts\", coalesce(col(\"ingest_ts\"), current_timestamp()))\n",
    "\n",
    "df.show(truncate=False)\n",
    "\n",
    "df.describe().show()\n",
    "\n",
    "# Deduplicate: keep the record with the max(ingest_ts) per order_id\n",
    "# w = Window.partitionBy(\"order_id\").orderBy(col(\"ingest_ts\").desc_nulls_last())\n",
    "latest_df = df.withColumn(\"_row_num\", expr(\"row_number() over (partition by order_id order by ingest_ts desc)\")) \\\n",
    "              .filter(col(\"_row_num\") == 1) \\\n",
    "              .drop(\"_row_num\")\n",
    "\n",
    "# return latest_df\n",
    "\n",
    "latest_df.show(truncate=False)\n",
    "\n",
    "latest_df.describe().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52050161-5b58-4e59-a8af-d2c8d6e5fdf6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
